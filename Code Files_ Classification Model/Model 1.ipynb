{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11028710,"sourceType":"datasetVersion","datasetId":6868322}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Group 5\n#Julia Aptekar, DePaul University, japtekar@depaul.edu\n#John Leniart, DePaul University, jleniart@depaul.edu\n#Arham Mehdi, DePaul University kmehdi@depaul.edu\n#Natalie Olechno, DePaul University, nolechno@depaul.edu\n\nimport pandas as pd \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, classification_report\nimport re\n\nfile_path = \"/kaggle/input/impact-genome/Combined Data.xlsx\"\ndata = pd.read_excel(file_path)\n\ndata = data.dropna(subset=['programdescription', 'outcomeid'])\n\ndata.fillna({'impactarea': 'unknown', 'genome': 'unknown'}, inplace=True)\n\nid_lst = [168, 186, 3238, 3461, 3473, 362, 3677, 3744, 3746, 3770, \n          3794, 4012, 4250, 453, 464, 471, 4815, 6917, 7555, 780, 877]\n\ndata.drop(data[data['programreportid'].isin(id_lst)].index, inplace=True)\n\n# Converting all text columns to lowercase for consistency\nfor col in ['programdescription', 'impactarea', 'genome', 'outcomeid']:\n    data[col] = data[col].astype(str).str.lower()\n\n# Mapping old outcome IDs to the larger (current) outcome IDs.\noutcome_id_mapping = {\n    \"625\": \"861\",  # Current Financial Stability\n    \"626\": \"863\",  # Financial Access\n    \"78\": \"860\",   # Financial Resilience\n    \"53\": \"859\"    # Future Security\n}\nmask = (data['impactarea'] == 'economic development') & (data['genome'] == 'financial health')\ndata.loc[mask, 'outcomeid'] = data.loc[mask, 'outcomeid'].replace(outcome_id_mapping)\n\n# Cleaning program descriptions (remove special characters)\ndata['programdescription'] = data['programdescription'].apply(lambda x: re.sub(r\"[^A-Za-z0-9 :.,'-]+\", \"\", x))\n\n# Encode categorical labels into numerical values\nlabel_encoders = {}\nfor col in ['impactarea', 'genome', 'outcomeid']:\n    le = LabelEncoder()\n    data[col] = le.fit_transform(data[col])\n    label_encoders[col] = le\n\n# Storing class counts\nimpactarea_classes = len(label_encoders['impactarea'].classes_)\ngenome_classes = len(label_encoders['genome'].classes_)\noutcomeid_classes = len(label_encoders['outcomeid'].classes_)\n\n# Here I define Dataset and Model\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        return encoding.input_ids.squeeze(0), encoding.attention_mask.squeeze(0), torch.tensor(self.labels[idx])\n\nclass BertClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BertClassifier, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n\n# Training and Evaluation Functions\ndef train_model(model, train_loader, num_epochs=3, lr=2e-5, class_weights=None):\n    \"\"\"\n    Trains a BERT-based classifier using CrossEntropy loss and AdamW optimizer.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for input_ids, attention_mask, labels in train_loader:\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n\ndef evaluate_model(model, test_loader):\n    \"\"\"\n    Evaluates the model and prints accuracy and classification report.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in test_loader:\n            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n            outputs = model(input_ids, attention_mask)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels.numpy())\n\n    accuracy = accuracy_score(true_labels, predictions)\n    print(f\"Accuracy: {accuracy}\\n{classification_report(true_labels, predictions)}\")\n    return accuracy\n\n# Tokenizer initialization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef train_and_predict(feature, target, num_classes, epochs):\n    \"\"\"\n    Trains and predicts using a hierarchical approach.\n    \"\"\"\n    if target == 'outcomeid':\n        # No stratification for outcomeid due to rare classes\n        X_train, X_test, y_train, y_test = train_test_split(\n            data[feature], data[target], test_size=0.2, random_state=42, stratify=None\n        )\n    else:\n        # Stratified sampling for impactarea and genome\n        X_train, X_test, y_train, y_test = train_test_split(\n            data[feature], data[target], test_size=0.2, random_state=42, stratify=data[target]\n        )\n\n    # Compute class weights using the full dataset\n    class_weights = compute_class_weight('balanced', classes=np.unique(data[target]), y=data[target])\n    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\n    train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n    test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n    model = BertClassifier(num_classes=num_classes)\n\n    train_model(model, train_loader, num_epochs=epochs, class_weights=class_weights_tensor)\n    evaluate_model(model, test_loader)\n\n    # Predict on the entire dataset\n    predicted_labels = []\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    with torch.no_grad():\n        for text in data[feature]:\n            enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n            enc_input_ids = enc.input_ids.to(device)\n            enc_attention_mask = enc.attention_mask.to(device)\n            output = model(enc_input_ids, enc_attention_mask)\n            predicted_labels.append(torch.argmax(output, dim=1).item())\n\n    data[f'predicted_{target}'] = predicted_labels\n    return model\n\n# Step 1: Predict Impact Area from Program Description\nimpactarea_model = train_and_predict('programdescription', 'impactarea', impactarea_classes, epochs=4)\n\n# Step 2: Predict Genome using both Program Description + Predicted Impact Area\ndata['predicted_impactarea'] = data['predicted_impactarea'].astype(str)\ndata['genome_features'] = data['programdescription'] + \" \" + data['predicted_impactarea']\ngenome_model = train_and_predict('genome_features', 'genome', genome_classes, epochs=5)\n\n# Step 3: Predict OutcomeID using Program Description + Predicted Impact Area + Predicted Genome\ndata['predicted_genome'] = [str(g) for g in data['predicted_genome']]\ndata['outcomeid_features'] = data['programdescription'] + \" \" + data['predicted_impactarea'] + \" \" + data['predicted_genome']\noutcomeid_model = train_and_predict('outcomeid_features', 'outcomeid', outcomeid_classes, epochs=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T17:53:23.283578Z","iopub.execute_input":"2025-03-18T17:53:23.283908Z","iopub.status.idle":"2025-03-18T18:34:17.634499Z","shell.execute_reply.started":"2025-03-18T17:53:23.283883Z","shell.execute_reply":"2025-03-18T18:34:17.633807Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4, Loss: 0.9764552333863019\nEpoch 2/4, Loss: 0.3954422038557125\nEpoch 3/4, Loss: 0.2522857382213113\nEpoch 4/4, Loss: 0.1794920494604677\nAccuracy: 0.8730378578024007\n              precision    recall  f1-score   support\n\n           0       0.89      0.85      0.87        84\n           1       0.87      0.95      0.91        21\n           2       0.92      0.90      0.91       333\n           3       0.92      0.95      0.93        93\n           4       0.89      0.84      0.87       396\n           5       0.89      0.86      0.87       353\n           6       0.92      0.88      0.90       153\n           7       0.96      0.93      0.94       351\n           8       0.81      0.86      0.84        44\n           9       0.97      0.94      0.95        88\n          10       0.56      0.69      0.62       116\n          11       0.71      0.86      0.77       134\n\n    accuracy                           0.87      2166\n   macro avg       0.86      0.88      0.86      2166\nweighted avg       0.88      0.87      0.88      2166\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.665037598557138\nEpoch 2/5, Loss: 0.9646851142174203\nEpoch 3/5, Loss: 0.546162434527135\nEpoch 4/5, Loss: 0.37897252636477075\nEpoch 5/5, Loss: 0.2772877818227804\nAccuracy: 0.863342566943675\n              precision    recall  f1-score   support\n\n           0       0.82      0.92      0.87        53\n           1       0.91      1.00      0.95        10\n           2       1.00      1.00      1.00         6\n           3       0.94      0.89      0.91        35\n           4       0.96      0.94      0.95        85\n           5       0.64      0.66      0.65        58\n           6       0.88      0.88      0.88        17\n           7       0.92      0.95      0.93        37\n           8       0.87      0.94      0.90        48\n           9       0.87      0.86      0.86       122\n          10       0.88      0.77      0.82        30\n          11       1.00      1.00      1.00        10\n          12       0.80      0.80      0.80         5\n          13       0.92      0.97      0.95        36\n          14       0.85      0.89      0.87        46\n          15       0.80      0.93      0.86        42\n          16       1.00      1.00      1.00        31\n          17       0.75      0.92      0.83        26\n          18       0.93      0.89      0.91       112\n          19       0.94      1.00      0.97        15\n          20       0.91      0.90      0.90       124\n          21       0.83      1.00      0.91         5\n          22       0.88      0.88      0.88        42\n          23       0.91      1.00      0.95        10\n          24       0.98      0.97      0.98       168\n          25       0.58      0.54      0.56        39\n          26       0.75      0.73      0.74        37\n          27       0.49      0.58      0.53        50\n          28       1.00      0.90      0.95        10\n          29       0.82      0.90      0.86        10\n          30       0.96      0.81      0.88        27\n          31       0.91      0.91      0.91        57\n          32       1.00      0.95      0.97        20\n          33       0.87      0.73      0.79        73\n          34       0.86      1.00      0.92        18\n          35       0.96      0.92      0.94        48\n          36       0.93      0.93      0.93        15\n          37       0.92      0.75      0.83        16\n          38       0.96      0.96      0.96        26\n          39       0.64      0.88      0.74        33\n          40       0.93      0.87      0.90        15\n          41       0.74      0.77      0.75        52\n          42       0.95      0.95      0.95        22\n          43       0.81      0.71      0.76        24\n          44       0.78      0.83      0.80        64\n          45       0.88      0.88      0.88        16\n          46       0.67      0.80      0.73         5\n          47       0.94      0.88      0.91        17\n          48       0.89      0.82      0.85       165\n          49       0.86      0.84      0.85       134\n\n    accuracy                           0.86      2166\n   macro avg       0.87      0.88      0.87      2166\nweighted avg       0.87      0.86      0.86      2166\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8, Loss: 5.2756373987866505\nEpoch 2/8, Loss: 4.207019878489505\nEpoch 3/8, Loss: 3.3973206224476717\nEpoch 4/8, Loss: 2.730222543227277\nEpoch 5/8, Loss: 2.197706792847257\nEpoch 6/8, Loss: 1.7506494203176886\nEpoch 7/8, Loss: 1.4044801908445534\nEpoch 8/8, Loss: 1.1284356823266652\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.6634349030470914\n              precision    recall  f1-score   support\n\n           0       0.80      0.67      0.73         6\n           1       0.70      0.88      0.78         8\n           2       0.50      1.00      0.67         1\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         7\n           6       0.83      0.83      0.83         6\n           7       0.71      0.38      0.50        13\n           8       0.83      1.00      0.91         5\n           9       0.75      0.50      0.60         6\n          10       0.86      0.75      0.80         8\n          11       1.00      0.75      0.86         4\n          12       1.00      1.00      1.00         4\n          14       0.47      0.28      0.35        29\n          15       0.80      0.80      0.80         5\n          16       0.20      0.29      0.24         7\n          17       0.83      1.00      0.91         5\n          18       0.67      0.50      0.57         8\n          19       0.80      1.00      0.89         4\n          20       0.56      0.83      0.67         6\n          21       1.00      1.00      1.00        10\n          22       1.00      1.00      1.00         5\n          23       1.00      1.00      1.00         7\n          24       0.47      0.67      0.55        12\n          25       1.00      0.57      0.73         7\n          26       0.75      0.27      0.40        11\n          27       1.00      0.29      0.45        17\n          28       1.00      1.00      1.00         7\n          29       1.00      1.00      1.00         2\n          30       1.00      1.00      1.00         4\n          31       0.83      0.83      0.83         6\n          32       0.71      0.56      0.63         9\n          33       1.00      0.80      0.89         5\n          34       1.00      0.83      0.91         6\n          35       1.00      0.83      0.91         6\n          36       0.80      0.67      0.73         6\n          37       0.83      1.00      0.91         5\n          39       0.50      1.00      0.67         4\n          40       0.25      0.25      0.25         4\n          41       0.83      1.00      0.91         5\n          42       0.09      0.14      0.11         7\n          43       1.00      1.00      1.00         6\n          44       0.62      0.23      0.33        22\n          45       1.00      0.80      0.89         5\n          46       0.50      0.75      0.60         4\n          47       1.00      0.12      0.22         8\n          48       1.00      0.83      0.91         6\n          49       0.00      0.00      0.00         1\n          50       0.67      0.86      0.75         7\n          51       0.33      0.31      0.32        13\n          52       0.86      1.00      0.92         6\n          53       0.83      0.83      0.83         6\n          54       1.00      1.00      1.00         2\n          55       0.00      0.00      0.00         5\n          56       0.75      0.75      0.75         4\n          57       1.00      0.90      0.95        10\n          58       0.86      1.00      0.92         6\n          59       1.00      0.83      0.91         6\n          60       0.29      1.00      0.44         4\n          61       1.00      1.00      1.00         7\n          62       1.00      1.00      1.00         3\n          63       0.67      0.80      0.73         5\n          64       0.57      1.00      0.73         4\n          65       0.80      0.57      0.67         7\n          66       0.50      0.33      0.40         9\n          67       1.00      1.00      1.00         7\n          68       0.75      0.27      0.40        11\n          69       0.83      1.00      0.91         5\n          70       0.56      0.37      0.45        51\n          71       0.82      0.82      0.82        11\n          72       1.00      0.80      0.89        10\n          73       1.00      1.00      1.00         6\n          74       0.55      0.83      0.66        29\n          75       0.44      0.80      0.57         5\n          76       0.46      0.60      0.52        10\n          77       0.44      0.15      0.23        26\n          78       1.00      0.78      0.88         9\n          79       0.67      0.50      0.57         8\n          80       0.00      0.00      0.00        11\n          81       0.50      0.75      0.60         4\n          82       0.50      0.67      0.57        12\n          83       0.30      0.60      0.40         5\n          84       0.78      0.86      0.82        37\n          86       0.83      1.00      0.91         5\n          87       0.83      1.00      0.91         5\n          88       1.00      0.04      0.07        53\n          89       1.00      1.00      1.00         8\n          90       1.00      1.00      1.00         7\n          91       1.00      1.00      1.00         6\n          92       1.00      0.83      0.91         6\n          93       0.83      1.00      0.91         5\n          94       0.67      1.00      0.80         4\n          95       1.00      1.00      1.00         3\n          96       1.00      1.00      1.00         7\n          97       0.83      0.83      0.83         6\n          98       1.00      1.00      1.00         3\n          99       0.00      0.00      0.00         1\n         100       0.50      0.25      0.33         4\n         101       0.18      0.67      0.29         3\n         102       1.00      0.75      0.86         4\n         103       1.00      1.00      1.00         5\n         104       1.00      0.80      0.89         5\n         105       0.70      0.43      0.53        37\n         106       1.00      1.00      1.00         6\n         107       0.80      1.00      0.89         4\n         108       1.00      1.00      1.00         4\n         109       1.00      0.86      0.92         7\n         110       0.00      0.00      0.00         6\n         111       1.00      1.00      1.00         4\n         113       0.80      0.80      0.80         5\n         114       1.00      1.00      1.00         5\n         115       0.00      0.00      0.00         1\n         116       0.77      0.67      0.71        15\n         118       0.75      0.50      0.60         6\n         119       1.00      1.00      1.00         5\n         120       1.00      0.40      0.57         5\n         121       0.66      0.86      0.75        22\n         122       1.00      0.75      0.86         4\n         123       0.59      0.62      0.61        16\n         124       1.00      0.75      0.86         4\n         125       0.67      0.67      0.67         6\n         126       1.00      1.00      1.00         4\n         127       0.60      1.00      0.75         3\n         128       1.00      1.00      1.00         8\n         129       0.67      1.00      0.80         4\n         130       1.00      1.00      1.00         6\n         131       0.44      0.67      0.53        12\n         132       1.00      1.00      1.00         8\n         133       0.00      0.00      0.00         1\n         134       1.00      1.00      1.00         2\n         135       1.00      0.83      0.91         6\n         136       1.00      1.00      1.00         5\n         137       0.50      0.20      0.29         5\n         138       0.91      0.91      0.91        11\n         139       0.70      1.00      0.82         7\n         140       0.50      0.67      0.57         3\n         141       0.88      1.00      0.93         7\n         142       1.00      1.00      1.00         5\n         143       0.75      0.50      0.60         6\n         144       1.00      0.90      0.95        10\n         145       1.00      1.00      1.00         4\n         146       1.00      1.00      1.00         6\n         147       0.56      0.83      0.67         6\n         148       0.57      0.80      0.67         5\n         149       1.00      1.00      1.00         3\n         150       0.56      1.00      0.71         5\n         151       1.00      1.00      1.00         4\n         152       0.60      0.60      0.60         5\n         153       0.00      0.00      0.00         1\n         154       0.67      0.25      0.36         8\n         155       0.75      0.75      0.75         4\n         156       0.17      1.00      0.30         4\n         157       1.00      1.00      1.00         9\n         158       1.00      0.64      0.78        11\n         159       1.00      1.00      1.00         1\n         160       0.15      0.67      0.25         3\n         161       0.67      1.00      0.80         4\n         162       1.00      1.00      1.00         6\n         163       1.00      0.62      0.77         8\n         164       1.00      1.00      1.00         6\n         165       1.00      1.00      1.00         3\n         166       1.00      1.00      1.00         5\n         167       1.00      0.88      0.93         8\n         168       1.00      1.00      1.00         6\n         169       1.00      0.50      0.67         2\n         170       1.00      0.67      0.80         3\n         171       1.00      1.00      1.00         3\n         172       1.00      1.00      1.00         3\n         173       0.83      1.00      0.91         5\n         175       1.00      0.17      0.29         6\n         176       0.75      1.00      0.86         3\n         177       0.80      0.80      0.80         5\n         180       0.67      1.00      0.80         2\n         181       1.00      1.00      1.00        10\n         183       0.86      1.00      0.92         6\n         184       0.77      0.89      0.83        19\n         186       0.50      0.43      0.46         7\n         187       0.67      0.33      0.44         6\n         188       0.75      1.00      0.86         6\n         189       1.00      1.00      1.00         5\n         190       0.57      0.80      0.67         5\n         191       1.00      1.00      1.00         3\n         192       0.60      1.00      0.75         3\n         193       0.83      1.00      0.91         5\n         194       0.75      0.86      0.80         7\n         195       0.31      0.48      0.38        25\n         196       0.00      0.00      0.00         2\n         197       0.00      0.00      0.00         6\n         198       0.50      0.33      0.40         3\n         199       0.83      0.36      0.50        14\n         200       0.43      1.00      0.60         3\n         201       0.67      0.57      0.62         7\n         202       1.00      1.00      1.00         3\n         203       1.00      1.00      1.00         5\n         204       0.00      0.00      0.00         3\n         205       0.38      1.00      0.55         3\n         206       0.33      1.00      0.50         3\n         207       1.00      1.00      1.00         3\n         208       0.49      0.86      0.62        21\n         209       0.75      1.00      0.86         3\n         210       0.81      0.57      0.67        51\n         211       1.00      1.00      1.00         1\n         212       1.00      1.00      1.00         2\n         214       1.00      1.00      1.00         6\n         215       0.71      0.83      0.77         6\n         217       0.00      0.00      0.00         0\n         218       1.00      0.67      0.80         3\n         219       1.00      1.00      1.00         7\n         220       1.00      1.00      1.00         2\n         221       1.00      0.67      0.80         6\n         222       0.75      1.00      0.86         3\n         223       0.75      1.00      0.86         3\n         224       0.73      0.35      0.48        31\n         225       0.45      0.42      0.43        12\n         226       0.80      0.80      0.80         5\n         227       1.00      1.00      1.00         4\n         228       1.00      1.00      1.00         8\n         229       0.67      0.67      0.67         6\n         230       1.00      1.00      1.00         2\n         231       0.40      0.67      0.50         3\n         232       0.20      1.00      0.33         1\n         233       0.40      0.40      0.40         5\n         234       0.83      0.83      0.83         6\n         235       0.50      0.83      0.62         6\n         236       0.60      0.67      0.63        36\n         237       0.37      0.50      0.42        14\n         238       1.00      1.00      1.00         3\n         239       0.67      0.13      0.22        15\n         240       0.86      1.00      0.92         6\n         241       0.19      0.33      0.24         9\n         242       0.36      0.40      0.38        10\n         243       0.36      0.50      0.42        10\n         244       0.50      0.75      0.60         4\n         245       0.83      0.83      0.83         6\n         246       0.50      0.30      0.37        10\n         247       0.67      0.33      0.44         6\n         248       0.45      0.83      0.59         6\n         249       0.00      0.00      0.00         1\n         250       0.38      0.50      0.43        16\n         251       0.60      0.60      0.60         5\n         252       0.50      0.60      0.55         5\n         253       1.00      0.50      0.67         2\n         254       0.43      0.60      0.50        10\n         255       0.29      0.67      0.40         3\n         256       1.00      1.00      1.00         5\n         257       0.60      0.50      0.55         6\n         258       0.29      0.67      0.40         9\n         259       0.29      0.62      0.40         8\n         260       0.40      0.80      0.53         5\n         261       1.00      0.12      0.22         8\n         262       0.14      0.38      0.20         8\n         263       1.00      0.71      0.83         7\n         264       1.00      1.00      1.00         3\n         265       0.88      1.00      0.93         7\n         266       0.29      1.00      0.44         2\n         267       0.50      1.00      0.67         4\n         268       1.00      1.00      1.00         5\n         269       0.00      0.00      0.00        10\n         270       0.53      0.96      0.68        67\n         271       0.50      0.60      0.55         5\n         272       0.83      1.00      0.91         5\n         273       0.00      0.00      0.00        13\n         274       0.00      0.00      0.00        10\n         275       0.80      0.80      0.80         5\n         277       0.93      0.83      0.88        47\n         278       0.70      0.64      0.67        11\n         279       1.00      1.00      1.00         8\n         280       1.00      1.00      1.00         4\n         281       1.00      0.12      0.21        17\n         282       1.00      1.00      1.00         5\n         283       0.21      0.45      0.29        11\n         284       0.85      0.70      0.77        47\n         285       1.00      0.75      0.86         4\n         286       0.44      0.50      0.47         8\n\n    accuracy                           0.66      2166\n   macro avg       0.72      0.73      0.70      2166\nweighted avg       0.71      0.66      0.65      2166\n\n","output_type":"stream"}],"execution_count":2}]}